{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a39961-ef88-4c98-bdaf-32599b4e4106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-time Data Processing with Azure Databricks and Event Hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ebf4ce5-030d-4066-94d2-87653411c8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376948f-980e-4887-a319-45a525172d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49397d51-5412-4079-9159-06307954a7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.sql(\"create catalog streaming;\")\n",
    "except:\n",
    "    print('check if catalog already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.bronze;\")\n",
    "except:\n",
    "    print('check if bronze schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.silver\")\n",
    "except:\n",
    "    print('check if silver schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.gold;\")\n",
    "except:\n",
    "    print('check if gold schema already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d00239-3c32-486b-bc93-377d054ba1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3397f5f1-841b-4911-9099-8b99b92ab7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connectionString = \"Listen accesss policy connection string of event hub\"\n",
    "eventHubName = \"Name of event hub\"\n",
    "\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n",
    "  'eventhubs.eventHubName': eventHubName\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab57af3-36c7-4ffa-b13f-a833d7a2dc53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load() \\\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/bronze/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.bronze.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbee364e-6b7e-420f-8b98-8a12d663d880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b661190-f917-448b-9093-1017dde3bb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = StructType([\n",
    "    StructField(\"temperature\", IntegerType()),\n",
    "    StructField(\"humidity\", IntegerType()),\n",
    "    StructField(\"windSpeed\", IntegerType()),\n",
    "    StructField(\"windDirection\", StringType()),\n",
    "    StructField(\"precipitation\", IntegerType()),\n",
    "    StructField(\"conditions\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db15d2-f3d1-4dd7-b5aa-ef421208b46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"skipChangeCommits\", \"true\")\\\n",
    "    .table(\"streaming.bronze.weather\")\\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\"))\\\n",
    "    .withColumn(\"body\",from_json(col(\"body\"), json_schema))\\\n",
    "    .select(\"body.temperature\", \"body.humidity\", \"body.windSpeed\", \"body.windDirection\", \"body.precipitation\", \"body.conditions\", col(\"enqueuedTime\").alias('timestamp'))\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/silver/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.silver.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4078a144-9652-490b-adf3-348ad8780e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4b337b-6bb7-490d-b1d3-2e8b83456661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"skipChangeCommits\", \"true\")\\\n",
    "    .table(\"streaming.silver.weather\")\\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .agg(avg(\"temperature\").alias('temperature'), avg(\"humidity\").alias('humidity'), avg(\"windSpeed\").alias('windSpeed'), avg(\"precipitation\").alias('precipitation'))\\\n",
    "\t.select('window.start', 'window.end', 'temperature', 'humidity', 'windSpeed', 'precipitation')\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/gold/weather_summary\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.gold.weather_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6453f5-cdeb-4e34-a2f8-a18d20839075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import json\n",
    "\n",
    "send_connection_string = \"send access policy connection string of event hub\"\n",
    "eventHubName = \"mp-car-sales-event-hub\"\n",
    "producer = EventHubProducerClient.from_connection_string(send_connection_string, eventhub_name=eventHubName)\n",
    "\n",
    "NUM_EVENTS_PER_BATCH = 100\n",
    "NUM_WORKERS = 20 \n",
    "TOTAL_EVENTS = 10000000 \n",
    "PARTITIONS = [\"NW\", \"NE\", \"SE\", \"SW\", \"EE\"]\n",
    "\n",
    "def generate_event(i, partition):\n",
    "    \"\"\"Generate a JSON event\"\"\"\n",
    "    return json.dumps({\n",
    "        \"temperature\": random.randint(-10, 40),\n",
    "        \"humidity\": random.randint(10, 100),\n",
    "        \"windSpeed\": random.randint(0, 50),\n",
    "        \"windDirection\": partition,\n",
    "        \"precipitation\": random.randint(0, 5),\n",
    "        \"conditions\": random.choice([\"Sunny\", \"Rainy\", \"Cloudy\", \"Stormy\"])\n",
    "    })\n",
    "\n",
    "async def send_batch(batch_events):\n",
    "    \"\"\"Send a batch of events asynchronously\"\"\"\n",
    "    try:\n",
    "        event_data_batch = producer.create_batch()\n",
    "        for event in batch_events:\n",
    "            try:\n",
    "                event_data_batch.add(EventData(event)) \n",
    "            except ValueError:\n",
    "                break\n",
    "\n",
    "        await producer.send_batch(event_data_batch)\n",
    "        print(f\"Sent batch of {len(batch_events)} events\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending batch: {e}\")\n",
    "\n",
    "async def send_events():\n",
    "    \"\"\"Send millions of events using high-concurrency\"\"\"\n",
    "    tasks = []\n",
    "    for _ in range(TOTAL_EVENTS // NUM_EVENTS_PER_BATCH):\n",
    "        batch_events = [generate_event(i, random.choice(PARTITIONS)) for i in range(NUM_EVENTS_PER_BATCH)]\n",
    "        tasks.append(send_batch(batch_events))\n",
    "\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    await send_events()\n",
    "    await producer.close()\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3add932e-55fa-48f3-a02c-657d78a92728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5899654777097620,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-time Data Processing with Azure Databricks and Event Hubs - Git",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
